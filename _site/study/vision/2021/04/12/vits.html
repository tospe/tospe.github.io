<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>PS: An Image is Worth 16x16 words: transformers for image recognition at scale | Tomas Sousa-Pereira</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="PS: An Image is Worth 16x16 words: transformers for image recognition at scale" />
<meta name="author" content="pudim" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Paper link: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE" />
<meta property="og:description" content="Paper link: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE" />
<meta property="og:site_name" content="Tomas Sousa-Pereira" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-12T00:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="PS: An Image is Worth 16x16 words: transformers for image recognition at scale" />
<script type="application/ld+json">
{"headline":"PS: An Image is Worth 16x16 words: transformers for image recognition at scale","dateModified":"2021-04-12T00:00:00+01:00","datePublished":"2021-04-12T00:00:00+01:00","author":{"@type":"Person","name":"pudim"},"mainEntityOfPage":{"@type":"WebPage","@id":"/study/vision/2021/04/12/vits.html"},"url":"/study/vision/2021/04/12/vits.html","description":"Paper link: AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE","@type":"BlogPosting","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Tomas Sousa-Pereira" />
<script src="/assets/js/mermaid.min.js"></script>
<script>
   mermaid.initialize({});

</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Tomas Sousa-Pereira</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/categories/">Categories</a><a class="page-link" href="/Projects/">Projects</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">PS: An Image is Worth 16x16 words: transformers for image recognition at scale</h1>
    <p class="post-meta"><time class="dt-published" datetime="2021-04-12T00:00:00+01:00" itemprop="datePublished">
        Apr 12, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Paper link: <a href="https://arxiv.org/pdf/2010.11929.pdf">AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</a>
<!-- What topic of paper / hypothesis / objectives -->
<!-- ### What --></p>

<!-- Why is this topic important (related) -->
<!-- ### Why -->

<!-- Metodology -->
<!-- ### How -->

<!-- results -->
<!-- ### Results -->

<!-- thoughts? conclusion -->
<!-- ### Conclusion -->

<h3 id="what">What</h3>
<p>Transforms have become the choice in natural language processing. In computer vision CNN still are the dominant, classic resnet like architetures still are the state of the art. Idea: is to apply a standard transformer directly to images. The image is splited in patches and provide the sequence of linear embeddings of these patches as input to a Transformer. ViT does not generalize well when trained on insufficient amounts of data.</p>

<h3 id="why">Why</h3>
<p>Transformers were proposed for machine learning translation, and have since becomestate of the art method in manu NLP tasks. Large transformer models are often pre-trained on large corpora and then fine tuned for the task at hand. Naive application of sel-attention to images would requre that each pixelattends to every other pixel, does not scale to realistic inputs. Many aproximations were tried, many demonstrate promising results, but complex engineering to be implemented efficiently. There is interest in combining CNN with forms with self-attention. Only one model (trained in a unserpervised fashion) that uses Transformers with global self-attention to full images, wich applies Transformers to images pixels after reducing image resolution and color space.</p>

<h3 id="how">How</h3>
<p><img src="/assets/images/vit.png" alt="ViT architecture" /></p>

<p>2D image is flattened. Similar to BERT [class] token x_class is a learnable embedding. The classification head is implemented by a MLP with one hidden layer at pre-training timeand by a single linear layer at fine-tuning. 1D learnable embeddings, since no performance gains from using 2D-aware position embeddings.</p>

<p>The transformer encoder consists of alternating layers of multiheaded self-attention. ViT was pre-trained on large datasets and fine-tune to smallerdownstream tasks. Remove the pre-trained prediction head and attach a zero-initialized feedorward layer. Is often beneficial to fine-tune at higher resolution than pre-training. When feeding higher resolution images, since the patch size is the same this means larger effective sequences.</p>

<p><strong>Models</strong>: ResNet, ViT and Hybrid (CNN). Pre training on different datasets of varying size and evaluate manu benchmark tasks. ViT attains state of the art on most recognition benchmarks at lower pre training cost. Self supervision in ViT holds promise for the future.</p>

<p><strong>Datasets</strong>: ImageNEt 1k classes and 1.3M images, ImageNet-21k 21k classesand 14M images, JFT 18K classes and 303M high-resolution images. Transfer models to ImageNEt Validation labels and the cleaned-up ReaL labels.</p>

<p><strong>Model Variants</strong>: ViT based on BERT. “Base” and “Large” directly adopted from BERT, added new “Huge” model. Transformers sequence length is inversely proportional to the square of the patch size thus models with smaller patch size are computationally more expensive. Baseline CNN, was used ResNet with Group Normalization and Used standardized convolutions.</p>

<h3 id="results">Results</h3>
<p>ViT-L/16 pre-trained on JFT-300M outperforms BiT-L on all tasks while requiring lesscomputational resources to train. ViT-H/14 further improves the performance. ViT-/16 model pre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking fewer resources to pre-train (TPUv3 for 30 days.</p>

<h3 id="conclusion">Conclusion</h3>
<p>First work with no Self-attention in computer vision. Instead the image is interpreted as a sequence of patches and process it by a standard transformer. Is Scalable and works really well when coupled with pre-training on large datasets. Matching and exceeding the state of the art on many images classification datasets, whilst being relatively cheap to pre-train.</p>

<p>Self-supervised pre-training methods need to be explored.</p>


  </div><a class="u-url" href="/study/vision/2021/04/12/vits.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">pudim</li>
          <li><a class="u-email" href="mailto:tospe.ami@gmail.com">tospe.ami@gmail.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Trying to learn new things.
</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://gitlab.com/tomassp" title="tomassp"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#gitlab"></use></svg></a></li><li><a rel="me" href="https://github.com/tospe" title="tospe"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
